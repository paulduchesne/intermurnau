{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge, add labelling to work, then add labelling to agent and manifestations.\n",
    "# then merge manifestations based on label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-16 07:28:15.527503 42006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# naive merge of triples\n",
    "\n",
    "import rdflib, pathlib, pydash, uuid\n",
    "import pandas, unidecode, json, numpy\n",
    "import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def concat_graph(address, graph_a):\n",
    "    graph_b = rdflib.Graph().parse(str(address), format=\"turtle\")\n",
    "    graph_a += graph_b\n",
    "    return graph_a\n",
    "\n",
    "graph = rdflib.Graph()\n",
    "graph = concat_graph(pathlib.Path.cwd().resolve().parents[0] / '1-ontology' / 'fiaf.ttl', graph) \n",
    "# for x in ['eye', 'nfa', 'sfi', 'moma', 'loc', 'barch', 'afa']:\n",
    "\n",
    "for x in ['sfi', 'moma', 'eye', 'nfa', 'barch', 'loc']:\n",
    "    graph = concat_graph(pathlib.Path.cwd().resolve().parents[0] / '2-contributors' / x / f'{x}.ttl', graph) \n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-16 07:28:16.207484 230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_a</th>\n",
       "      <th>SUBJECT_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>https://loc.gov/resource/work/1208391</td>\n",
       "      <td>https://www.bundesarchiv.de/resource/work/5431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>https://www.nfa.cz/resource/work/0201933</td>\n",
       "      <td>https://www.bundesarchiv.de/resource/work/77258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>https://www.nfa.cz/resource/work/0201933</td>\n",
       "      <td>https://www.eyefilm.nl/resource/work/FLM65135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>https://www.nfa.cz/resource/work/0201933</td>\n",
       "      <td>https://www.moma.org/resource/work/W2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>https://www.nfa.cz/resource/work/0201933</td>\n",
       "      <td>https://loc.gov/resource/work/1675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     SUBJECT_a  \\\n",
       "1537     https://loc.gov/resource/work/1208391   \n",
       "1726  https://www.nfa.cz/resource/work/0201933   \n",
       "1717  https://www.nfa.cz/resource/work/0201933   \n",
       "1714  https://www.nfa.cz/resource/work/0201933   \n",
       "1698  https://www.nfa.cz/resource/work/0201933   \n",
       "\n",
       "                                            SUBJECT_b  \n",
       "1537   https://www.bundesarchiv.de/resource/work/5431  \n",
       "1726  https://www.bundesarchiv.de/resource/work/77258  \n",
       "1717    https://www.eyefilm.nl/resource/work/FLM65135  \n",
       "1714         https://www.moma.org/resource/work/W2770  \n",
       "1698               https://loc.gov/resource/work/1675  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# merging works, currently performed by just exploiting title similarities\n",
    "\n",
    "\n",
    "name_wb = rdflib.Namespace('http://wikibas.se/ontology')\n",
    "name_fiaf = rdflib.Namespace(\"https://www.fiafnet.org/\")\n",
    "\n",
    "def pull_subject(p, o):\n",
    "    table = pandas.DataFrame(columns=['SUBJECT'])\n",
    "    for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], o)):\n",
    "        for d,e,f in graph.triples((None, name_wb['#claim'], a)):\n",
    "            table.loc[len(table)] = [(d)]            \n",
    "    return table\n",
    "\n",
    "def pull_property(p):\n",
    "    table = pandas.DataFrame(columns=['SUBJECT', 'OBJECT'])\n",
    "    for a,b,c in graph.triples((None, p, None)):\n",
    "        for d,e,f in graph.triples((None, name_wb['#claim'], a)):\n",
    "            table.loc[len(table)] = [(d), (c)]\n",
    "    return table\n",
    "\n",
    "data_a = pull_subject(name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work']) \n",
    "data_b = pull_property(name_fiaf['ontology/property/title'])\n",
    "works = pandas.merge(data_a, data_b, on='SUBJECT', how='left').drop_duplicates()    \n",
    "\n",
    "works['hinge'] = '.'\n",
    "works = pandas.merge(works.rename(columns={'SUBJECT':'SUBJECT_a', 'OBJECT':'OBJECT_a'}), \n",
    "                     works.rename(columns={'SUBJECT':'SUBJECT_b', 'OBJECT':'OBJECT_b'}),\n",
    "                     on='hinge', how='outer')\n",
    "\n",
    "def title_matching(row):\n",
    "    left = unidecode.unidecode(row['OBJECT_a']).upper()\n",
    "    right = unidecode.unidecode(row['OBJECT_b']).upper()\n",
    "    ratio = fuzz.partial_ratio(left, right)\n",
    "    return ratio\n",
    "\n",
    "works['fuzz'] = works.apply(title_matching, axis=1)\n",
    "works = works.sort_values(by='fuzz', ascending=False)\n",
    "works = works.loc[works.fuzz > 85].drop_duplicates()\n",
    "works = works[['SUBJECT_a', 'SUBJECT_b']]\n",
    "works = works.loc[works.SUBJECT_a != works.SUBJECT_b].drop_duplicates()\n",
    "\n",
    "owl_graph = rdflib.Graph()\n",
    "\n",
    "for x in range(len(works)):\n",
    "    section = works.iloc[x]\n",
    "    owl_graph.add((section['SUBJECT_a'], rdflib.OWL.sameAs, section['SUBJECT_b']))\n",
    "\n",
    "print(datetime.datetime.now(), len(works))\n",
    "works.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-16 07:28:37.603159 42006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# restart process, # create a list \n",
    "# can you link all synonyms and then do a replacer\n",
    "# what I want to do is string or things, where if there is any way to link it can be done\n",
    "\n",
    "trails = list()\n",
    "for s,p,o in owl_graph:\n",
    "    trails.append([s,o])\n",
    "\n",
    "for k in range(27):    \n",
    "    for n, t in enumerate(trails):\n",
    "        for x in trails:\n",
    "            if set(t).intersection(set(x)):\n",
    "                trails[n].extend(x)\n",
    "        trails[n] = sorted(pydash.uniq(trails[n]))   \n",
    "\n",
    "graph_a = graph\n",
    "\n",
    "for t in pydash.uniq(trails):\n",
    "    graph_b = rdflib.Graph()\n",
    "    new_uri = rdflib.URIRef(name_fiaf[f'resource/{str(uuid.uuid4())}'])\n",
    "    \n",
    "    for s,p,o in graph_a:\n",
    "        if s in t:\n",
    "            s = new_uri\n",
    "        if p in t:\n",
    "            p = new_uri            \n",
    "        if o in t:\n",
    "            o = new_uri   \n",
    "        graph_b.add((s,p,o))\n",
    "    graph_a = graph_b\n",
    "    \n",
    "graph = graph_a\n",
    "        \n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-16 07:30:00.836712 42025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add major title as work label\n",
    "\n",
    "work_dataframe = pandas.DataFrame(columns=['a','b','c']) # 47 instances of works (from claim)\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work'])):\n",
    "    work_dataframe.loc[len(work_dataframe)] = [(a),(b),(c)]\n",
    "    \n",
    "claim_dataframe = pandas.DataFrame(columns=['a','b','c']) # 7004 claims in total\n",
    "for a,b,c in graph.triples((None, name_wb['#claim'], None)):\n",
    "    claim_dataframe.loc[len(claim_dataframe)] = [(a),(b),(c)]\n",
    "    \n",
    "ref_dataframe = pandas.DataFrame(columns=['c','d','e']) # 5307 reference claims\n",
    "for a,b,c in graph.triples((None, name_wb['#reference'], None)):\n",
    "    ref_dataframe.loc[len(ref_dataframe)] = [(a),(b),(c)]    \n",
    "    \n",
    "contrib_dataframe = pandas.DataFrame(columns=['e','f','institute']) # \n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/contributed_by'], None)):\n",
    "    contrib_dataframe.loc[len(contrib_dataframe)] = [(a),(b),(c)]    \n",
    "    \n",
    "contrib_dataframe = pandas.merge(ref_dataframe, contrib_dataframe, on='e', how='right')  # 5307, a is the claim, e is the institute\n",
    "contrib_dataframe = contrib_dataframe[['c', 'institute']].drop_duplicates()\n",
    "     \n",
    "work_subject = claim_dataframe.loc[claim_dataframe.c.isin(list(work_dataframe.a))] # 47, so this is all claims which turn into instance of work\n",
    "work_claim = claim_dataframe.loc[claim_dataframe.a.isin(list(work_subject.a))] # 1121 all claims from \"the works\"\n",
    "    \n",
    "title_dataframe = pandas.DataFrame(columns=['c','d','e']) # all title claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/title'], None)):\n",
    "    title_dataframe.loc[len(title_dataframe)] = [(a),(b),(c)]  \n",
    "    \n",
    "title_dataframe = pandas.merge(work_claim, title_dataframe, on='c', how='right') # this is the guy, now we need to blend in referneces\n",
    "title_dataframe = pandas.merge(title_dataframe, contrib_dataframe, on='c', how='left') # this is the guy, now we need to blend in referneces\n",
    "title_dataframe = title_dataframe[['a', 'e', 'institute']].drop_duplicates()\n",
    "title_dataframe = title_dataframe.pivot_table(index=['a','e'], aggfunc=lambda x: len(x.unique())).reset_index()\n",
    "title_dataframe = title_dataframe.sort_values(by='institute', ascending=False)\n",
    "title_dataframe = title_dataframe.drop_duplicates(subset='a', keep='first')\n",
    "\n",
    "# # okay now we can write english lables for each of these, hooray\n",
    "\n",
    "for x in range(len(title_dataframe)):\n",
    "    line = title_dataframe.iloc[x]\n",
    "    graph.add((line['a'], rdflib.RDFS.label, rdflib.Literal(line['e'], lang='en'))) \n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "64\n",
      "3056\n",
      "2021-01-16 07:31:13.017530 42025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # can you tap into the same system for agents?\n",
    "\n",
    "# okay this is not accurate enough, you need to drill down by work, and by agent-type to find \"likes\"\n",
    "# you could probs have different ratios (eg crew can be loose, but cast has to be a bit more strict)\n",
    "\n",
    "def pull_direct(g, p, o):\n",
    "    table = pandas.DataFrame(columns=['SUBJECT'])\n",
    "    for a,b,c in g.triples((None, p, o)):\n",
    "        table.loc[len(table)] = [(a)]            \n",
    "    return table\n",
    "\n",
    "def pull_subject(g, p, o):\n",
    "    table = pandas.DataFrame(columns=['SUBJECT'])\n",
    "    for a,b,c in g.triples((None, p, o)):\n",
    "        for d,e,f in g.triples((None, name_wb['#claim'], a)):\n",
    "            table.loc[len(table)] = [(d)]            \n",
    "    return table\n",
    "\n",
    "def pull_qual(g, p, col):\n",
    "    table = pandas.DataFrame(columns=['CLAIM', col])\n",
    "    for a,b,c in g.triples((None, p, None)):\n",
    "        for d,e,f in g.triples((None, name_wb['#qualifier'], a)):\n",
    "            table.loc[len(table)] = [(d), (c)]            \n",
    "    return table\n",
    "\n",
    "def pull_property_claim(g, p, col, col2):\n",
    "    table = pandas.DataFrame(columns=['SUBJECT', col, col2])\n",
    "    for a,b,c in g.triples((None, p, None)):\n",
    "        for d,e,f in g.triples((None, name_wb['#claim'], a)):\n",
    "            table.loc[len(table)] = [(d), (c), (f)]\n",
    "    return table\n",
    "\n",
    "def pull_property(p, col2, col1):\n",
    "    table = pandas.DataFrame(columns=[col1, col2])\n",
    "    for a,b,c in graph.triples((None, p, None)):\n",
    "        for d,e,f in graph.triples((None, name_wb['#claim'], a)):\n",
    "            table.loc[len(table)] = [(d), (c)]\n",
    "    return table\n",
    "\n",
    "\n",
    "agents = pull_property_claim(graph, name_fiaf['ontology/property/agent'], 'AGENT', 'CLAIM') \n",
    "agent_qual = pull_qual(graph, name_fiaf['ontology/property/agent_type'], 'AGENT_TYPE')\n",
    "agent_dataframe = pandas.merge(agents, agent_qual, on='CLAIM', how='left')\n",
    "\n",
    "forename = pull_property(name_fiaf['ontology/property/forename'], 'FORENAME', 'AGENT').drop_duplicates()\n",
    "agent_dataframe = pandas.merge(agent_dataframe, forename, on='AGENT', how='left')\n",
    "\n",
    "surname = pull_property(name_fiaf['ontology/property/surname'], 'SURNAME', 'AGENT').drop_duplicates()\n",
    "agent_dataframe = pandas.merge(agent_dataframe, surname, on='AGENT', how='left')\n",
    "\n",
    "\n",
    "# okay now you can carve up based on agent type and work and see how you go\n",
    "\n",
    "\n",
    "agent_type = pull_direct(graph, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/agent_type']) \n",
    "print(len(agent_type))\n",
    "works = pull_subject(graph, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work']) \n",
    "print(len(works))\n",
    "\n",
    "owl_graph = rdflib.Graph()\n",
    "\n",
    "# for a in [x for x in list(agent_type.SUBJECT)]:\n",
    "#     for w in pydash.uniq([x for x in list(works.SUBJECT)]):\n",
    "# #         print(a, w)\n",
    "        \n",
    "section = agent_dataframe.copy()\n",
    "#         section = section.loc[section.SUBJECT.isin([w])]\n",
    "#         section = section.loc[section.AGENT_TYPE.isin([a])] \n",
    "section = section[['AGENT', 'FORENAME', 'SURNAME']].drop_duplicates() # now hinge this!\n",
    "section['FORENAME'] = section['FORENAME'].str.replace('.','').str.upper()\n",
    "section['SURNAME'] = section['SURNAME'].str.replace('.','').str.upper()        \n",
    "\n",
    "\n",
    "\n",
    "section['hinge'] = '.'\n",
    "\n",
    "section = pandas.merge(section.rename(columns={'AGENT':'AGENT_a', 'FORENAME':'FORENAME_a', 'SURNAME':'SURNAME_a'}), \n",
    "             section.rename(columns={'AGENT':'AGENT_b', 'FORENAME':'FORENAME_b', 'SURNAME':'SURNAME_b' }),\n",
    "             on='hinge', how='outer')\n",
    "\n",
    "# if len(section):\n",
    "\n",
    "def matching(row, a, b):\n",
    "    ratio = fuzz.partial_ratio(str(row[a]), str(row[b]))\n",
    "    return ratio\n",
    "\n",
    "section['fuzz_forename'] = section.apply(matching, a = 'FORENAME_a', b = 'FORENAME_b', axis=1)        \n",
    "section['fuzz_surname'] = section.apply(matching, a = 'SURNAME_a', b = 'SURNAME_b', axis=1)  \n",
    "\n",
    "section['fuzz_mean'] = (section['fuzz_forename']+section['fuzz_surname'])/2\n",
    "\n",
    "\n",
    "# agents = agents.sort_values(by='fuzz', ascending=False)\n",
    "\n",
    "section = section.loc[section.AGENT_a != section.AGENT_b].drop_duplicates()\n",
    "section = section.loc[section.fuzz_mean >= 80].drop_duplicates()\n",
    "section = section[['AGENT_a', 'AGENT_b']]\n",
    "\n",
    "section = section.loc[section.AGENT_a != section.AGENT_b].drop_duplicates()\n",
    "\n",
    "for x in range(len(section)):\n",
    "    sect = section.iloc[x]\n",
    "    owl_graph.add((sect['AGENT_a'], rdflib.OWL.sameAs, sect['AGENT_b']))\n",
    "\n",
    "print(len(owl_graph))\n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8ec3298e5c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mtrails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# restart process, # create a list \n",
    "# can you link all synonyms and then do a replacer\n",
    "# what I want to do is string or things, where if there is any way to link it can be done\n",
    "\n",
    "trails = list()\n",
    "for s,p,o in owl_graph:\n",
    "    trails.append([s,o])\n",
    "\n",
    "for k in range(27):    \n",
    "    for n, t in enumerate(trails):\n",
    "        for x in trails:\n",
    "            if set(t).intersection(set(x)):\n",
    "                trails[n].extend(x)\n",
    "        trails[n] = sorted(pydash.uniq(trails[n]))   \n",
    "\n",
    "graph_a = graph\n",
    "\n",
    "for t in pydash.uniq(trails):\n",
    "    graph_b = rdflib.Graph()\n",
    "    new_uri = rdflib.URIRef(name_fiaf[f'resource/{str(uuid.uuid4())}'])\n",
    "    \n",
    "    for s,p,o in graph_a:\n",
    "        if s in t:\n",
    "            s = new_uri\n",
    "        if p in t:\n",
    "            p = new_uri            \n",
    "        if o in t:\n",
    "            o = new_uri   \n",
    "        graph_b.add((s,p,o))\n",
    "    graph_a = graph_b\n",
    "    \n",
    "    \n",
    "graph = graph_a\n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# and here we do agent labels\n",
    "# agent labels are a bit more complex, because you actually have to run twice for fore and sur\n",
    "\n",
    "agent_dataframe = pandas.DataFrame(columns=['a','b','c']) # 746 instances of agent (from claim)\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/agent'])):\n",
    "    agent_dataframe.loc[len(agent_dataframe)] = [(a),(b),(c)]\n",
    "    \n",
    "claim_dataframe = pandas.DataFrame(columns=['a','b','c']) # 7004 claims in total\n",
    "for a,b,c in graph.triples((None, name_wb['#claim'], None)):\n",
    "    claim_dataframe.loc[len(claim_dataframe)] = [(a),(b),(c)]\n",
    "    \n",
    "# ref_dataframe = pandas.DataFrame(columns=['c','d','e']) # 5307 reference claims\n",
    "# for a,b,c in graph.triples((None, name_wb['#reference'], None)):\n",
    "#     ref_dataframe.loc[len(ref_dataframe)] = [(a),(b),(c)]    \n",
    "    \n",
    "# contrib_dataframe = pandas.DataFrame(columns=['e','f','institute']) # \n",
    "# for a,b,c in graph.triples((None, name_fiaf['ontology/property/contributed_by'], None)):\n",
    "#     contrib_dataframe.loc[len(contrib_dataframe)] = [(a),(b),(c)]    \n",
    "    \n",
    "# contrib_dataframe = pandas.merge(ref_dataframe, contrib_dataframe, on='e', how='right')  # 5307, a is the claim, e is the institute\n",
    "# contrib_dataframe = contrib_dataframe[['c', 'institute']].drop_duplicates()\n",
    "     \n",
    "agent_subject = claim_dataframe.loc[claim_dataframe.c.isin(list(agent_dataframe.a))] # 746, so this is all claims which turn into instance of work\n",
    "agent_claim = claim_dataframe.loc[claim_dataframe.a.isin(list(agent_subject.a))] # 3684 all claims from \"the works\"\n",
    "    \n",
    "fore_dataframe = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/forename'], None)):\n",
    "    fore_dataframe.loc[len(fore_dataframe)] = [(a),(b),(c)]  \n",
    "    \n",
    "fore_dataframe = pandas.merge(agent_claim, fore_dataframe, on='c', how='right') # 731 first names\n",
    "fore_dataframe = pandas.merge(fore_dataframe, contrib_dataframe, on='c', how='left') # this is the guy, now we need to blend in referneces\n",
    "fore_dataframe = fore_dataframe[['a', 'e', 'institute']].drop_duplicates()\n",
    "fore_dataframe = fore_dataframe.pivot_table(index=['a','e'], aggfunc=lambda x: len(x.unique())).reset_index()\n",
    "fore_dataframe = fore_dataframe.sort_values(by='institute', ascending=False)\n",
    "fore_dataframe = fore_dataframe.drop_duplicates(subset='a', keep='first') # 264 down\n",
    "fore_dataframe = fore_dataframe[['a','e']].rename(columns={'e':'forename'})\n",
    "\n",
    "surn_dataframe = pandas.DataFrame(columns=['c','d','e']) # all sur claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/surname'], None)):\n",
    "    surn_dataframe.loc[len(surn_dataframe)] = [(a),(b),(c)]  \n",
    "    \n",
    "surn_dataframe = pandas.merge(agent_claim, surn_dataframe, on='c', how='right') # 731 first names\n",
    "surn_dataframe = pandas.merge(surn_dataframe, contrib_dataframe, on='c', how='left') # this is the guy, now we need to blend in referneces\n",
    "surn_dataframe = surn_dataframe[['a', 'e', 'institute']].drop_duplicates()\n",
    "surn_dataframe = surn_dataframe.pivot_table(index=['a','e'], aggfunc=lambda x: len(x.unique())).reset_index()\n",
    "surn_dataframe = surn_dataframe.sort_values(by='institute', ascending=False)\n",
    "surn_dataframe = surn_dataframe.drop_duplicates(subset='a', keep='first') # 273 down\n",
    "surn_dataframe = surn_dataframe[['a','e']].rename(columns={'e':'surname'})\n",
    "\n",
    "names_dataframe = pandas.merge(fore_dataframe, surn_dataframe, on='a', how='outer').fillna('')\n",
    "\n",
    "# # # okay now we can write english lables for each of these, hooray\n",
    "\n",
    "for x in range(len(names_dataframe)):\n",
    "    line = names_dataframe.iloc[x]\n",
    "    spliced = (line['forename']+' '+line['surname']).strip()\n",
    "    graph.add((line['a'], rdflib.RDFS.label, rdflib.Literal(spliced, lang='en'))) \n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# okay now lets do manifestation labels. way this works is go down to item level,\n",
    "# and then return gauge, base, \n",
    "\n",
    "# base, sepcific carrier, carrier\n",
    "\n",
    "manifest_dataframe = pandas.DataFrame(columns=['a','b','c']) # 156 manifests\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/manifestation'])):\n",
    "    manifest_dataframe.loc[len(manifest_dataframe)] = [(a),(b),(c)]\n",
    "\n",
    "claim_dataframe = pandas.DataFrame(columns=['a','b','c']) # 7004 claims in total\n",
    "for a,b,c in graph.triples((None, name_wb['#claim'], None)):\n",
    "    claim_dataframe.loc[len(claim_dataframe)] = [(a),(b),(c)]\n",
    "        \n",
    "    \n",
    "manifest_subject = claim_dataframe.loc[claim_dataframe.c.isin(list(manifest_dataframe.a))] # 156 manifests\n",
    "manifest_claim = claim_dataframe.loc[claim_dataframe.a.isin(list(manifest_subject.a))] # 468 totals, now we want to follow down to items\n",
    "\n",
    "# print(manifest_claim.b.unique())\n",
    "    \n",
    "item_property = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/item'], None)):\n",
    "    item_property.loc[len(item_property)] = [(a),(b),(c)]      \n",
    "    \n",
    "item_property = pandas.merge(manifest_claim, item_property, on='c', how='right') # 156 items first names  \n",
    "\n",
    "secondary_claim = claim_dataframe.copy()\n",
    "secondary_claim.columns = ['e','f','g']\n",
    "item_property = pandas.merge(item_property, secondary_claim, on='e', how='left')\n",
    "\n",
    "# first grab, specific carrier\n",
    "sc_property = pandas.DataFrame(columns=['g','h','i']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/specific_carrier'], None)):\n",
    "    sc_property.loc[len(sc_property)] = [(a),(b),(c)]  \n",
    "sc_property = pandas.merge(item_property, sc_property, on='g', how='right') # 123 bases items first names   \n",
    "sc_property = sc_property[['a','i']].drop_duplicates().rename(columns={'i':'sc'})\n",
    "\n",
    "# second grab, base\n",
    "base_property = pandas.DataFrame(columns=['g','h','i']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/base'], None)):\n",
    "    base_property.loc[len(base_property)] = [(a),(b),(c)]  \n",
    "base_property = pandas.merge(item_property, base_property, on='g', how='right') # 123 bases items first names   \n",
    "base_property = base_property[['a','i']].drop_duplicates().rename(columns={'i':'b'})\n",
    "\n",
    "# third grab, carrier\n",
    "carr_property = pandas.DataFrame(columns=['g','h','i']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/carrier'], None)):\n",
    "    carr_property.loc[len(carr_property)] = [(a),(b),(c)]  \n",
    "carr_property = pandas.merge(item_property, carr_property, on='g', how='right') # 123 bases items first names   \n",
    "carr_property = carr_property[['a','i']].drop_duplicates().rename(columns={'i':'c'})\n",
    "\n",
    "manifestation_rename = pandas.merge(sc_property, base_property, on='a', how='outer')\n",
    "manifestation_rename = pandas.merge(manifestation_rename, carr_property, on='a', how='outer')\n",
    "\n",
    "\n",
    "manifestation_property = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/manifestation_of'], None)):\n",
    "    manifestation_property.loc[len(manifestation_property)] = [(a),(b),(c)] \n",
    "\n",
    "manifestation_property = pandas.merge(claim_dataframe, manifestation_property, on='c', how='right')\n",
    "\n",
    "labels = pandas.DataFrame(columns=['e','f','g']) # all fore claims\n",
    "for a,b,c in graph.triples((None, rdflib.RDFS.label, None)):\n",
    "    labels.loc[len(labels)] = [(a),(b),(c)] \n",
    "    \n",
    "manifestation_property = pandas.merge(labels, manifestation_property, on='e', how='right')  \n",
    "manifestation_property = manifestation_property[['a', 'g']].rename(columns={'g':'label'})\n",
    "\n",
    "manifestation_rename = pandas.merge(manifestation_property, manifestation_rename, on='a', how='left')\n",
    "\n",
    "# okay we need to add label as a column\n",
    "\n",
    "for x in range(len(manifestation_rename)):\n",
    "    line = manifestation_rename.iloc[x]\n",
    "    \n",
    "    spliced = str(line['sc']).split('/')[-1].replace('digibeta', 'Digibeta').replace('mxf', 'MXF').replace('quarter-inch', '1/4-inch')+' '\n",
    "    if line['b'] is not numpy.nan:\n",
    "        spliced += str(line['b']).split('/')[-1].title()+' '  \n",
    "    spliced += str(line['c']).split('/')[-1].replace('_','').title()+' '\n",
    "    spliced += f\"of {line['label']}.\"#     spliced\n",
    "    spliced = spliced.strip()\n",
    "    graph.add((line['a'], rdflib.RDFS.label, rdflib.Literal(spliced, lang='en'))) \n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# okay now attempt to join all like named manifestations\n",
    "# you really just need a two way hinged data frame\n",
    "\n",
    "manifest = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/manifestation'])):\n",
    "    manifest.loc[len(manifest)] = [(a),(b),(c)] \n",
    "    \n",
    "manifest = pandas.merge(claim_dataframe, manifest, on='c', how='right')    \n",
    "\n",
    "labels = pandas.DataFrame(columns=['a','label']) # all fore claims\n",
    "for a,b,c in graph.triples((None, rdflib.RDFS.label, None)):\n",
    "    labels.loc[len(labels)] = [(a),(c)] \n",
    "    \n",
    "manifest = pandas.merge(manifest, labels, on='a', how='left')    \n",
    "manifest = manifest[['a', 'label']].drop_duplicates().rename(columns={'a':'manifest'})\n",
    "\n",
    "manifest['hinge'] = '.'\n",
    "manifest = pandas.merge(manifest.rename(columns={'manifest':'manifest_a', 'label':'label_a'}), \n",
    "                     manifest.rename(columns={'manifest':'manifest_b', 'label':'label_b'}),\n",
    "                     on='hinge', how='outer')\n",
    "\n",
    "manifest = manifest.loc[manifest.manifest_a != manifest.manifest_b].drop_duplicates()\n",
    "manifest = manifest.loc[manifest.label_a == manifest.label_b].drop_duplicates()\n",
    "\n",
    "owl_graph = rdflib.Graph()\n",
    "for x in range(len(manifest)):\n",
    "    section = manifest.iloc[x]\n",
    "    owl_graph.add((section['manifest_a'], rdflib.OWL.sameAs, section['manifest_b']))\n",
    "    \n",
    "    \n",
    "# so this is the owl filtering code spliced directly - should really be a function huh?    \n",
    "    \n",
    "trails = list()\n",
    "for s,p,o in owl_graph:\n",
    "    trails.append([s,o])\n",
    "\n",
    "for k in range(27):    \n",
    "    for n, t in enumerate(trails):\n",
    "        for x in trails:\n",
    "            if set(t).intersection(set(x)):\n",
    "                trails[n].extend(x)\n",
    "        trails[n] = sorted(pydash.uniq(trails[n]))   \n",
    "\n",
    "graph_a = graph\n",
    "\n",
    "for t in pydash.uniq(trails):\n",
    "    graph_b = rdflib.Graph()\n",
    "    new_uri = rdflib.URIRef(name_fiaf[f'resource/{str(uuid.uuid4())}'])\n",
    "    \n",
    "    for s,p,o in graph_a:\n",
    "        if s in t:\n",
    "            s = new_uri\n",
    "        if p in t:\n",
    "            p = new_uri            \n",
    "        if o in t:\n",
    "            o = new_uri   \n",
    "        graph_b.add((s,p,o))\n",
    "    graph_a = graph_b\n",
    "    \n",
    "graph = graph_a\n",
    "        \n",
    "print(datetime.datetime.now(), len(graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LAST THING TO DO HERE, add some item labels which will just be institutuion and external id as label, easy\n",
    "# then write this to local wikibase.\n",
    "# then write this to online wikibase.\n",
    "\n",
    "\n",
    "item_labels = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/item'])):\n",
    "    item_labels.loc[len(item_labels)] = [(a),(b),(c)] \n",
    "    \n",
    "item_subject = claim_dataframe.loc[claim_dataframe.c.isin(list(item_labels.c))] # 156 manifests\n",
    "item_claim = claim_dataframe.loc[claim_dataframe.a.isin(list(item_subject.a))] # 1626 totals, now we want to follow down to items\n",
    "\n",
    "# okay so now we need to issue two requests, the external id and the held at\n",
    "\n",
    "\n",
    "held_dataframe = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/held_at'], None)):\n",
    "    held_dataframe.loc[len(held_dataframe)] = [(a),(b),(c)]  \n",
    "    \n",
    "held_dataframe = pandas.merge(item_claim, held_dataframe, on='c', how='right') # 731 first names\n",
    "held_dataframe = held_dataframe[['a','e']].drop_duplicates().rename(columns={'e':'institute'})\n",
    "\n",
    "exid_dataframe = pandas.DataFrame(columns=['c','d','e']) # all fore claims\n",
    "for a,b,c in graph.triples((None, name_fiaf['ontology/property/external_id'], None)):\n",
    "    exid_dataframe.loc[len(exid_dataframe)] = [(a),(b),(c)]  \n",
    "    \n",
    "exid_dataframe = pandas.merge(item_claim, exid_dataframe, on='c', how='right') # 731 first names\n",
    "exid_dataframe = exid_dataframe[['a','e']].drop_duplicates().rename(columns={'e':'external'})\n",
    "\n",
    "dataframe = pandas.merge(held_dataframe, exid_dataframe, on='a', how='left')\n",
    "\n",
    "def inst_rename(row):\n",
    "    if str(row['institute']) == 'https://www.nfa.cz/ontology/item/nfa':\n",
    "        return 'National Film Archive'\n",
    "    if str(row['institute']) == 'https://www.filminstitutet.se/ontology/item/sfi':\n",
    "        return 'Swedish Film Institute'\n",
    "    if str(row['institute']) == 'https://www.moma.org/ontology/item/moma':\n",
    "        return 'Museum of Modern Art'\n",
    "    if str(row['institute']) == 'https://www.eyefilm.nl/ontology/item/eye':\n",
    "        return 'Eye Film Institute'    \n",
    "    if str(row['institute']) == 'https://www.bundesarchiv.de/ontology/item/barch':\n",
    "        return 'Bundesarchiv' \n",
    "    if str(row['institute']) == 'https://loc.gov/ontology/item/loc':\n",
    "        return 'Library of Congress'     \n",
    "\n",
    "dataframe['institute'] = dataframe.apply(inst_rename, axis=1)\n",
    "\n",
    "print(dataframe.institute.unique())\n",
    "\n",
    "for x in range(len(dataframe)):\n",
    "    line = dataframe.iloc[x]\n",
    "    spliced = (line['institute']+' '+line['external']).strip()\n",
    "    graph.add((line['a'], rdflib.RDFS.label, rdflib.Literal(spliced, lang='en'))) \n",
    "\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try and link claims, same method as above, find matching claims\n",
    "# genius idea, can you do a pivot for claim ids?\n",
    "\n",
    "claim_dataframe = pandas.DataFrame(columns=['S', 'P', 'O', 'C'])\n",
    "\n",
    "for a,b,c in graph.triples((None, name_wb['#claim'], None)): # this is all yr claims\n",
    "    for d,e,f in graph.triples((c, None, None)):\n",
    "        claim_dataframe.loc[len(claim_dataframe)] = [(a), (e), (f), (c)]\n",
    "\n",
    "claim_dataframe = claim_dataframe.pivot_table(index=['S','P','O'], aggfunc=lambda x: ','.join(sorted(x.unique()))).reset_index()\n",
    "claim_dataframe = claim_dataframe.loc[claim_dataframe.C.str.contains(',', na=False)]\n",
    "claim_overlap = list(claim_dataframe.C)\n",
    "claim_overlap = [x.split(',') for x in claim_overlap]\n",
    "\n",
    "for n, x in enumerate(claim_overlap):\n",
    "    claim_overlap[n] = [rdflib.URIRef(y) for y in x]\n",
    "    \n",
    "graph_a = graph\n",
    "\n",
    "for t in pydash.uniq(claim_overlap):\n",
    "    graph_b = rdflib.Graph()\n",
    "    new_uri = rdflib.URIRef(name_fiaf[f'resource/{str(uuid.uuid4())}'])\n",
    "    \n",
    "    for s,p,o in graph_a:\n",
    "        if s in t:\n",
    "            s = new_uri\n",
    "        if p in t:\n",
    "            p = new_uri            \n",
    "        if o in t:\n",
    "            o = new_uri   \n",
    "        graph_b.add((s,p,o))\n",
    "    graph_a = graph_b\n",
    "        \n",
    "graph = graph_a\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph.serialize(destination=str(pathlib.Path.cwd() / 'merge.ttl'), format=\"turtle\")\n",
    "print(datetime.datetime.now(), len(graph))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
