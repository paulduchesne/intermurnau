{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "\n",
    "import rdflib, pandas, pathlib\n",
    "import numpy, uuid\n",
    "import pathlib, xmltodict, json\n",
    "import pydash, uuid, rdflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define graph and namespace\n",
    "\n",
    "graph = rdflib.Graph()\n",
    "\n",
    "name_nfa = rdflib.Namespace('https://www.nfa.cz/') \n",
    "name_wb = rdflib.Namespace('http://wikibas.se/ontology')\n",
    "name_fiaf = rdflib.Namespace(\"https://www.fiafnet.org/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define institution\n",
    "\n",
    "claim1 = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]\n",
    "graph.add((name_nfa['ontology/item/nfa'], rdflib.RDFS.label, rdflib.Literal('National Film Archive', lang='en'))) \n",
    "graph.add((name_nfa['ontology/item/nfa'], rdflib.RDFS.label, rdflib.Literal('Národní filmový archiv', lang='cs'))) \n",
    "\n",
    "graph.add((name_nfa['ontology/item/nfa'], name_wb['#claim'], claim1))\n",
    "graph.add((claim1, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/holding_institution'])) \n",
    "\n",
    "claim2 = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]\n",
    "graph.add((name_nfa['ontology/item/nfa'], name_wb['#claim'], claim2))\n",
    "graph.add((claim2, name_fiaf['ontology/property/located_in'], name_fiaf['ontology/item/czech_republic'])) \n",
    "\n",
    "print(len(graph)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define specific linking properties\n",
    "\n",
    "graph.add((name_nfa['ontology/property/nfa_work_id'], rdflib.RDFS.label, rdflib.Literal('National Film Archive work ID', lang='en'))) \n",
    "graph.add((name_nfa['ontology/property/nfa_agent_id'], rdflib.RDFS.label, rdflib.Literal('National Film Archive agent ID', lang='en'))) \n",
    "graph.add((name_nfa['ontology/property/nfa_item_id'], rdflib.RDFS.label, rdflib.Literal('National Film Archive item ID', lang='en'))) \n",
    "\n",
    "print(len(graph)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reference\n",
    "\n",
    "def reference(claim_id):\n",
    "    \n",
    "    ref_id = name_nfa[f\"resource/reference/{uuid.uuid4()}\"]\n",
    "    graph.add((claim_id, name_wb['#reference'], ref_id))\n",
    "    graph.add((ref_id, name_fiaf['ontology/property/contributed_by'], name_nfa['ontology/item/nfa']))  \n",
    "\n",
    "print(len(graph))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make claim\n",
    "\n",
    "def make_claim(s, p, o):        \n",
    "    claim_x = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]    \n",
    "    graph.add((s, name_wb['#claim'], claim_x))\n",
    "    graph.add((claim_x, p, o))\n",
    "    return claim_x\n",
    "\n",
    "print(len(graph))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load and normalise data\n",
    "\n",
    "import pathlib, xmltodict, json\n",
    "import pydash, uuid, rdflib\n",
    "\n",
    "pathway = pathlib.Path.cwd() / 'murnau_ais_export.xml'\n",
    "\n",
    "data = list()\n",
    "with open(pathway) as source_data:\n",
    "    source_data = source_data.read().split('</FILM>')\n",
    "    for d in source_data:\n",
    "        try:\n",
    "            data.append(xmltodict.parse(d+'</FILM>'))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "with open(pathlib.Path.cwd() / 'nfa.json', 'w') as nfa_json:\n",
    "    json.dump(data, nfa_json)\n",
    "\n",
    "print(len(graph))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write work specifics\n",
    "\n",
    "for x in data:\n",
    "    work_id = pydash.get(x, 'FILM.FILMID')\n",
    "    work = name_nfa[f\"resource/work/{work_id}\"]\n",
    "    \n",
    "#     claim1 = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]\n",
    "#     claim2 = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]    \n",
    "\n",
    "#     graph.add((work, name_wb['#claim'], claim1))\n",
    "#     graph.add((claim1, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work']))\n",
    "\n",
    "#     graph.add((work, name_wb['#claim'], claim2))\n",
    "#     graph.add((claim2, name_nfa['ontology/property/nfa_work_id'], rdflib.Literal(work_id))) \n",
    "    \n",
    "    \n",
    "#     work_id = x['ID']\n",
    "#     work = name_eye[f\"resource/work/{work_id}\"]\n",
    "    \n",
    "    make_claim(work, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work'])\n",
    "    make_claim(work, name_nfa['ontology/property/nfa_work_id'], rdflib.Literal(work_id))    \n",
    "    \n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "# write original title\n",
    "\n",
    "for x in data:\n",
    "    work_id = pydash.get(x, 'FILM.FILMID')\n",
    "    work = name_nfa[f\"resource/work/{work_id}\"]  \n",
    "    \n",
    "#     claim1 = name_nfa[f\"resource/claim/{uuid.uuid4()}\"]\n",
    "    qual1 = name_nfa[f\"resource/qualifier/{uuid.uuid4()}\"]    \n",
    "#     ref1 = name_nfa[f\"resource/reference/{uuid.uuid4()}\"]  \n",
    "    \n",
    "    title = str(pydash.get(x, 'FILM.NAZEV-ORIGIN')).split(',')\n",
    "    if len(title) > 1:\n",
    "        title = title[1].strip()+' '+title[0].strip()\n",
    "    else:\n",
    "        title = title[0]    \n",
    "        \n",
    "    claim1 = make_claim(work, name_fiaf['ontology/property/title'], rdflib.Literal(title))        \n",
    "\n",
    "#     graph.add((work, name_wb['#claim'], claim1))\n",
    "#     graph.add((claim1, name_fiaf['ontology/property/title'], rdflib.Literal(title)))\n",
    "        \n",
    "    graph.add((claim1, name_wb['#qualifier'], qual1))\n",
    "    graph.add((qual1, name_fiaf['ontology/property/title_type'], name_fiaf['ontology/item/original_title']))\n",
    "    \n",
    "    reference(claim1)\n",
    "            \n",
    "#     graph.add((claim1, name_wb['#reference'], ref1))\n",
    "#     graph.add((ref1, name_fiaf['ontology/property/contributed_by'], name_nfa['ontology/item/nfa']))  \n",
    "            \n",
    "print(len(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def write_credit(work_data, dict_key, agent_type): \n",
    "\n",
    "    work_id = pydash.get(x, 'FILM.FILMID')\n",
    "    work = name_nfa[f\"resource/work/{work_id}\"] \n",
    "    if pydash.get(x, f'FILM.{dict_key}') != None:\n",
    "#         print(type(pydash.get(x, f'FILM.{dict_key}')))\n",
    "        \n",
    "        if isinstance(pydash.get(x, f'FILM.{dict_key}'), list):\n",
    "            agent_data = pydash.get(x, f'FILM.{dict_key}')\n",
    "#             print('list')\n",
    "        else:\n",
    "            agent_data = [pydash.get(x, f'FILM.{dict_key}')]\n",
    "#             print('nt t list')\n",
    "        \n",
    "        for o in agent_data:\n",
    "            \n",
    "#             print(work_id, dict_key)\n",
    "\n",
    "            agent1 = name_nfa[f\"resource/agent/{uuid.uuid4()}\"]\n",
    "            claim1 = make_claim(work, name_fiaf['ontology/property/agent'], agent1)        \n",
    "            reference(claim1)            \n",
    "\n",
    "            qual1 = name_nfa[f\"resource/qualifier/{uuid.uuid4()}\"]  \n",
    "            graph.add((claim1, name_wb['#qualifier'], qual1))\n",
    "            graph.add((qual1, name_fiaf['ontology/property/agent_type'], agent_type))\n",
    "\n",
    "            make_claim(agent1, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/agent'])  \n",
    "            \n",
    "            if dict_key == 'OBSAZENI':\n",
    "                fore = pydash.get(o, 'HEREC.JMENO')\n",
    "                sur = pydash.get(o, 'HEREC.PRIJMENI')\n",
    "            else:\n",
    "                fore = pydash.get(o, 'JMENO')\n",
    "                sur = pydash.get(o, 'PRIJMENI')     \n",
    "                \n",
    "#             print(fore, sur)    \n",
    "            \n",
    "            claim2 = make_claim(agent1, name_fiaf['ontology/property/forename'], rdflib.Literal(fore))         \n",
    "            reference(claim2)      \n",
    "            claim3 = make_claim(agent1, name_fiaf['ontology/property/surname'], rdflib.Literal(sur))         \n",
    "            reference(claim3) \n",
    "            \n",
    "            make_claim(agent1, name_fiaf['ontology/property/work'], work)            \n",
    "        \n",
    "for x in data:\n",
    "    \n",
    "    write_credit(x, 'OBSAZENI', name_fiaf['ontology/item/cast'])\n",
    "    write_credit(x, 'REZIE', name_fiaf['ontology/item/director']) \n",
    "    write_credit(x, 'KAMERA', name_fiaf['ontology/item/cinematographer'])       \n",
    "    write_credit(x, 'SCENAR', name_fiaf['ontology/item/screenwriter'])  \n",
    "    write_credit(x, 'HUDBA', name_fiaf['ontology/item/composer'])      \n",
    "        \n",
    "print(len(graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# questions, will need to find eye-like item info, request via email\n",
    "# you can still build in items though\n",
    "# also is there any interesting additional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph.serialize(destination=str(pathlib.Path.cwd() / 'nfa.ttl'), format=\"turtle\")\n",
    "print(len(graph))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
