{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Academy Film Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "\n",
    "import rdflib, pandas, pathlib, json\n",
    "import numpy, uuid, xmltodict, pydash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define graph and namespace\n",
    "\n",
    "graph = rdflib.Graph()\n",
    "name_afa = rdflib.Namespace('https://www.oscars.org/film-archive') \n",
    "name_wb = rdflib.Namespace('http://wikibas.se/ontology')\n",
    "name_fiaf = rdflib.Namespace('https://www.fiafnet.org/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# useful functions\n",
    "\n",
    "def make_claim(s, p, o):        \n",
    "    claim_id = name_afa[f\"resource/claim/{uuid.uuid4()}\"]    \n",
    "    graph.add((s, name_wb['#claim'], claim_id))\n",
    "    graph.add((claim_id, p, o))\n",
    "    return claim_id\n",
    "\n",
    "def make_qual(s, p, o):\n",
    "    qual_id = name_afa[f\"resource/qualifier/{uuid.uuid4()}\"]       \n",
    "    graph.add((s, name_wb['#qualifier'], qual_id))\n",
    "    graph.add((qual_id, p, o))\n",
    "    return qual_id\n",
    "\n",
    "def reference(claim_id, institute):\n",
    "    ref_id = name_afa[f\"resource/reference/{uuid.uuid4()}\"]\n",
    "    graph.add((claim_id, name_wb['#reference'], ref_id))\n",
    "    graph.add((ref_id, name_fiaf['ontology/property/contributed_by'], institute))  \n",
    "    \n",
    "def single_list(data):  \n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        return [data]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define institution\n",
    "\n",
    "graph.add((name_afa['ontology/item/afa'], rdflib.RDFS.label, rdflib.Literal('Academy Film Archive', lang='en'))) \n",
    "make_claim(name_afa['ontology/item/afa'], name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/holding_institution'])\n",
    "make_claim(name_afa['ontology/item/afa'], name_fiaf['ontology/property/located_in'], name_fiaf['ontology/item/usa'])\n",
    "\n",
    "print(len(graph)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# format data\n",
    "\n",
    "with open(pathlib.Path.cwd() / 'AFA_murnau_Works_2021-01-13.xml') as work_xml:    \n",
    "    work_data = pydash.get(xmltodict.parse(work_xml.read()), 'adlibXML.recordList.record')\n",
    "\n",
    "with open(pathlib.Path.cwd() / 'AFA_murnau_items_carriers_2021-01-11.xml') as item_xml:    \n",
    "    item_data = pydash.get(xmltodict.parse(item_xml.read()), 'adlibXML.recordList.record')\n",
    "    \n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write work\n",
    "\n",
    "for x in work_data:\n",
    "    \n",
    "    work_id = x['object_number']\n",
    "    work = name_afa[f\"resource/work/{work_id}\"]\n",
    "    \n",
    "    make_claim(work, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/work'])\n",
    "    claim_id = make_claim(work, name_fiaf['ontology/property/external_id'], rdflib.Literal(work_id))\n",
    "    make_qual(claim_id, name_fiaf['ontology/property/institution'], name_afa['ontology/item/afa'])\n",
    "    reference(claim_id, name_afa['ontology/item/afa'])     \n",
    "\n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write title\n",
    "\n",
    "for x in work_data:\n",
    "\n",
    "    work_id = x['object_number']\n",
    "    work = name_afa[f\"resource/work/{work_id}\"]\n",
    "    \n",
    "    orig = [y for y in pydash.get(x, 'Title') if 'origin' in str(y)][0]\n",
    "    title = pydash.get(orig, 'title')\n",
    "\n",
    "    claim_id = make_claim(work, name_fiaf['ontology/property/title'], rdflib.Literal(title.strip()))\n",
    "    make_qual(claim_id, name_fiaf['ontology/property/title_type'], name_fiaf['ontology/item/original_title'])\n",
    "    reference(claim_id, name_afa['ontology/item/afa'])      \n",
    "\n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write country\n",
    "\n",
    "for x in work_data:\n",
    "    \n",
    "    work_id = x['object_number']\n",
    "    work = name_afa[f\"resource/work/{work_id}\"]\n",
    "        \n",
    "    for k, v in {'Germany':name_fiaf['ontology/item/germany'], 'United States':name_fiaf['ontology/item/usa']}.items():\n",
    "        if pydash.get(x, 'Production.production_country') == k:\n",
    "            claim_id = make_claim(work, name_fiaf['ontology/property/production_country'], v)  \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])        \n",
    "\n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write agent\n",
    "\n",
    "def write_credit(source, contribution, uri):\n",
    "\n",
    "    for s in [x for x in source if x['type'] == contribution]:\n",
    "    \n",
    "        work = s['work']\n",
    "        agent = name_afa[f\"resource/agent/{s['id']}\"]            \n",
    "\n",
    "        claim_id = make_claim(work, name_fiaf['ontology/property/agent'], agent)\n",
    "        make_qual(claim_id, name_fiaf['ontology/property/agent_type'], uri)                \n",
    "        reference(claim_id, name_afa['ontology/item/afa']) \n",
    "\n",
    "        make_claim(agent, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/agent']) \n",
    "\n",
    "        claim_id = make_claim(agent, name_fiaf['ontology/property/external_id'], rdflib.Literal(s['id']))\n",
    "        make_qual(claim_id, name_fiaf['ontology/property/institution'], name_afa['ontology/item/afa'])\n",
    "        reference(claim_id, name_afa['ontology/item/afa'])\n",
    "        \n",
    "        name = s['name'].split(',')\n",
    "\n",
    "        claim_id = make_claim(agent, name_fiaf['ontology/property/forename'], rdflib.Literal(name[1].strip()))\n",
    "        reference(claim_id, name_afa['ontology/item/afa'])        \n",
    "        \n",
    "        claim_id = make_claim(agent, name_fiaf['ontology/property/surname'], rdflib.Literal(name[0].strip()))\n",
    "        reference(claim_id, name_afa['ontology/item/afa']) \n",
    "\n",
    "        claim_id = make_claim(agent, name_fiaf['ontology/property/work'], work)    \n",
    "        reference(claim_id, name_afa['ontology/item/afa'])               \n",
    "\n",
    "combined = list()\n",
    "for x in work_data:\n",
    "    \n",
    "    work_id = x['object_number']\n",
    "    work = name_afa[f\"resource/work/{work_id}\"]    \n",
    "    \n",
    "    if 'credits' in x:\n",
    "        for y in pydash.get(x, 'credits'):\n",
    "            name = pydash.get(y, 'credit\\.name.name')\n",
    "            agent_id = pydash.get(y, 'credit\\.name\\.lref')\n",
    "            agent_type = pydash.get(y, 'credit\\.type')\n",
    "            combined.append({'work': work, 'id':agent_id, 'name':name, 'type':agent_type})\n",
    "        \n",
    "    if 'director_credit' in x:\n",
    "        director = single_list(pydash.get(x, 'director_credit'))\n",
    "        for y in director:\n",
    "            name = pydash.get(y, 'director.name')\n",
    "            agent_id = pydash.get(y, 'director\\.lref')\n",
    "            agent_type = pydash.get(y, 'director\\.type')\n",
    "            combined.append({'work': work, 'id':agent_id, 'name':name, 'type':agent_type})\n",
    "                  \n",
    "write_credit(combined, 'Cast', name_fiaf['ontology/item/cast'])                    \n",
    "write_credit(combined, 'Director', name_fiaf['ontology/item/director']) \n",
    "write_credit(combined, 'Writer', name_fiaf['ontology/item/screenwriter']) \n",
    "write_credit(combined, 'Producer', name_fiaf['ontology/item/producer']) \n",
    "write_credit(combined, 'Director of photography', name_fiaf['ontology/item/cinematographer']) \n",
    "write_credit(combined, 'Music', name_fiaf['ontology/item/composer'])  \n",
    "write_credit(combined, 'Editor', name_fiaf['ontology/item/editor'])      \n",
    "\n",
    "print(len(graph))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write manifestations/items\n",
    "\n",
    "manifestation_dict = dict()\n",
    "for x in work_data:\n",
    "    if 'Parts' in x:\n",
    "        for y in single_list(pydash.get(x, 'Parts')):\n",
    "            manif_id = y['parts_reference']\n",
    "            manifestation_dict[manif_id] = x['object_number']\n",
    "            \n",
    "items = list()\n",
    "for x in [i for i in item_data if i['object_number'][0] == 'I']:\n",
    "    work_id = manifestation_dict[pydash.get(x, 'Part_of.part_of_reference')]  \n",
    "    item_id = pydash.get(x, 'object_number')\n",
    "    \n",
    "    item_carrier = pydash.get(x, 'item_type.value.0.#text') # note video is not split\n",
    "    \n",
    "    video_carrier = pydash.get(x, 'material_type_video') # this has video tape and disc split\n",
    "    audio_carrier = pydash.get(x, 'material_type_audio') # this has audio tape\n",
    "    film_carrier = pydash.get(x, 'material_type_film') # this has safety or nitrate split    \n",
    "    \n",
    "    film_specific = pydash.get(x, 'material_format_film') # like release print\n",
    "    video_specific = pydash.get(x, 'material_format_video') # like digibeta\n",
    "    audio_specific = pydash.get(x, 'material_format_audio') # like 1/4\" tape \n",
    "    \n",
    "    gauge = pydash.get(x, 'gauge.gauge_film') # has film gauge but also other length\n",
    "    colour = pydash.get(x, 'image_color') # colour\n",
    "    base = pydash.get(x, 'base') # base \n",
    "    length = pydash.get(x, 'footage_length') # extent    \n",
    "    duration = pydash.get(x, 'duration') # duration\n",
    "    usage = pydash.get(x, 'copy_usage') # usage\n",
    "    \n",
    "    items.append({'work_id':work_id, 'item_id': item_id, 'item_carrier': item_carrier, 'video_carrier': video_carrier, \n",
    "                  'audio_carrier': audio_carrier, 'film_specific': film_specific, \n",
    "                  'video_specific': video_specific, 'audio_specific': audio_specific, 'gauge': gauge, \n",
    "                  'colour': colour, 'base': base, 'length': length, 'duration': duration, 'usage': usage})\n",
    "    \n",
    "print(len(graph))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# process manifestations/items\n",
    "\n",
    "for i in items:\n",
    "\n",
    "    work = name_afa[f\"resource/work/{i['work_id']}\"]\n",
    "\n",
    "    manifestation = name_afa[f\"resource/manifestation/{uuid.uuid4()}\"]\n",
    "    make_claim(manifestation, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/manifestation'])\n",
    "    make_claim(manifestation, name_fiaf['ontology/property/manifestation_of'], work)\n",
    "\n",
    "    item_id = i['item_id']\n",
    "    item = name_afa[f\"resource/item/{item_id}\"]     \n",
    "\n",
    "    make_claim(item, name_fiaf['ontology/property/instance_of'], name_fiaf['ontology/item/item'])        \n",
    "    make_claim(item, name_fiaf['ontology/property/item_of'], manifestation)  \n",
    "\n",
    "    claim_id = make_claim(item, name_fiaf['ontology/property/held_at'], name_afa['ontology/item/afa'])\n",
    "    reference(claim_id, name_afa['ontology/item/afa'])          \n",
    "\n",
    "    claim_id = make_claim(item, name_fiaf['ontology/property/external_id'], rdflib.Literal(item_id))\n",
    "    make_qual(claim_id, name_fiaf['ontology/property/institution'], name_afa['ontology/item/afa'])\n",
    "    reference(claim_id, name_afa['ontology/item/afa'])  \n",
    "\n",
    "    for k, v in {'FILM':name_fiaf['ontology/item/film'], \n",
    "                 'DIGITAL':name_fiaf['ontology/item/digital']}.items():\n",
    "        if i['item_carrier'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])  \n",
    "            \n",
    "    for k, v in {'Video Tape':name_fiaf['ontology/item/video_tape'], 'Disc':name_fiaf['ontology/item/disc']}.items():\n",
    "        if i['video_carrier'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])    \n",
    "            \n",
    "    for k, v in {'Audio Tape':name_fiaf['ontology/item/sound_tape']}.items():\n",
    "        if i['audio_carrier'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])             \n",
    "            \n",
    "    for k, v in {\n",
    "        'fine grain master positive': name_fiaf['ontology/item/duplicate_positive'], \n",
    "        'picture negative': name_fiaf['ontology/item/negative'],\n",
    "        'dupe picture negative': name_fiaf['ontology/item/duplicate_negative'], \n",
    "        'optical track negative': name_fiaf['ontology/item/sound_negative'],\n",
    "        'dupe negative': name_fiaf['ontology/item/duplicate_negative'], \n",
    "        'Work print': name_fiaf['ontology/item/work_print'],\n",
    "        'print': name_fiaf['ontology/item/print']}.items():\n",
    "        if i['film_specific'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/element'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa']) \n",
    "  \n",
    "    for k, v in {\n",
    "        'Betacam SP': name_fiaf['ontology/item/betacamsp'],\n",
    "        'Betamax': name_fiaf['ontology/item/betamax'],\n",
    "        'D5': name_fiaf['ontology/item/d5'],\n",
    "        'D2': name_fiaf['ontology/item/d2'],\n",
    "        'VHS': name_fiaf['ontology/item/vhs'],\n",
    "        'Hi-8mm': name_fiaf['ontology/item/hi8'],\n",
    "        'HDCAM SR': name_fiaf['ontology/item/hdcam'],\n",
    "        'Digital Betacam': name_fiaf['ontology/item/digibeta'],\n",
    "        'Digital Video Disk (DVD)': name_fiaf['ontology/item/dvd']}.items():\n",
    "        if i['video_specific'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/specific_carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])  \n",
    "\n",
    "    for k, v in {\n",
    "        '1/4\" Magnetic Tape': name_fiaf['ontology/item/quarter-inch']}.items():\n",
    "        if i['audio_specific'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/specific_carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])  \n",
    "\n",
    "    for k, v in {'16mm':name_fiaf['ontology/item/16mm'], '35mm':name_fiaf['ontology/item/35mm'],\n",
    "                 '8mm':name_fiaf['ontology/item/8mm']}.items():\n",
    "        if i['gauge'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/specific_carrier'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa']) \n",
    "                 \n",
    "    for k, v in {'color':name_fiaf['ontology/item/colour'], 'black and white':name_fiaf['ontology/item/black_and_white'],\n",
    "                 'black and white (tinted)':name_fiaf['ontology/item/tinted']}.items():\n",
    "        if i['colour'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/colour'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa']) \n",
    "                \n",
    "    for k, v in {'safety':name_fiaf['ontology/item/acetate'], 'acetate':name_fiaf['ontology/item/acetate'],\n",
    "                 'nitrate':name_fiaf['ontology/item/nitrate'], 'polyester':name_fiaf['ontology/item/polyester']}.items():\n",
    "        if i['base'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/base'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])                \n",
    "\n",
    "    for k, v in {'Preservation':name_fiaf['ontology/item/master'], 'Conservation':name_fiaf['ontology/item/master'],\n",
    "                 'Access':name_fiaf['ontology/item/viewing']}.items():\n",
    "        if i['usage'] == k:                       \n",
    "            claim_id = make_claim(item, name_fiaf['ontology/property/access'], v)   \n",
    "            reference(claim_id, name_afa['ontology/item/afa'])         \n",
    "    \n",
    "    if i['length']:\n",
    "        claim_id = make_claim(item, name_fiaf['ontology/property/extent_feet'], rdflib.Literal(i['length']))\n",
    "        reference(claim_id, name_afa['ontology/item/afa'])         \n",
    "             \n",
    "    if i['duration']:\n",
    "        claim_id = make_claim(item, name_fiaf['ontology/property/duration'], rdflib.Literal(i['duration']))\n",
    "        reference(claim_id, name_afa['ontology/item/afa'])              \n",
    " \n",
    "    make_claim(work, name_fiaf['ontology/property/manifestation'], manifestation)\n",
    "    make_claim(manifestation, name_fiaf['ontology/property/item'], item) \n",
    "\n",
    "print(len(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph.serialize(destination=str(pathlib.Path.cwd() / 'afa.ttl'), format=\"turtle\")\n",
    "print(len(graph))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
